{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from fancyimpute import KNN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pydotplus\n",
    "\n",
    "import collections\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.offline\n",
    "import plotly.graph_objs as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all samples from the file\n",
    "data_kihd = pd.read_excel('kihd_may_2019.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2682, 992)\n",
      "KIHD with genes only: (2682, 96)\n",
      "KIHD with phenotypes only: (2682, 977)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# Keep the original data in 'data_kihd' and use its copy 'data_kihd_preprocessed' instead\n",
    "# -----------------------------------------\n",
    "data_kihd_preprocessed = data_kihd.copy()\n",
    "\n",
    "# Drop two variables (dates of visits)\n",
    "data_kihd_preprocessed = data_kihd_preprocessed.drop(['tpvm2', 'tpnr2'], axis = 1)\n",
    "\n",
    "# Correct the zero-level for the following variables:\n",
    "wrong_zero = ['v0563', 'v0565', 'v0567', 'v0569', 'v0571', 'v0573', 'v0575', 'v0577', 'v0579', 'v0607', 'v0609', 'v0613',\n",
    "              'v0621', 'v0623', 'v0625', 'v0627', 'v0629', 'v0631', 'v0633', 'v0635', 'v0637', 'v0639', 'v0643', 'v0645', \n",
    "              'v0647', 'v0649', 'v0651', 'v0653']\n",
    "# Zeros are at the wrong end of the scale\n",
    "# Change zeros to (max+1)\n",
    "for col in wrong_zero:\n",
    "    data_kihd_preprocessed.loc[:, col] = [np.max(data_kihd_preprocessed.loc[:, col]) + 1 if value == 0.0 else value for value in data_kihd_preprocessed.loc[:, col]]\n",
    "\n",
    "print(data_kihd_preprocessed.shape)\n",
    "# -----------------------------------------\n",
    "# Turn the categorical variables indo dummies\n",
    "# -----------------------------------------\n",
    "categorical_variables=['au0136','au0153','ek0115','ek0119','ka0118','mi0205','mi0207','mi0208','mi0209',\n",
    "                       'mi0210','mi0211','mi0212','mi0213','mi0214','v0145','v0146','v0157','v0158','v0161',\n",
    "                       'v0172','v0247','v0248','v0665','v0721','v0724','u1307']\n",
    "\n",
    "for col in categorical_variables:\n",
    "    if col in data_kihd_preprocessed.columns:\n",
    "        new_dummies=pd.get_dummies(data_kihd_preprocessed[col], dummy_na=False)\n",
    "        my_list = new_dummies.columns.values\n",
    "        string = col+\"_\"\n",
    "        my_new_list = [string + str(x) for x in my_list]\n",
    "        new_dummies.columns = my_new_list\n",
    "        data_kihd_preprocessed = data_kihd_preprocessed.drop(col, axis=1)       \n",
    "        data_kihd_preprocessed = data_kihd_preprocessed.join(new_dummies)\n",
    "                 \n",
    "# Outcomes\n",
    "kihd_outcomes = data_kihd_preprocessed.loc[:, ['tutknro', 'chdb16', 'chdb16d', 'amif16', 'amif16d', 'amig16', 'amig16d',\n",
    "       'amim16', 'amim16d', 'all16', 'all16d', 'cvd16', 'cvd16d', 'syd14', 'alzm14', 'vp14', 'kol14', 'diab14',\n",
    "       'ncvd16', 'ncvd16d', 'cv15', 'cv15d', 'all15', 'stro15', 'cvd15', 'chd15', 'amib15', 'isth15', 'hsth15', \n",
    "                                'db15', 'can15', 'canc15', 'ast15', 'copd15', 'dema15', 'finchd18', 'finchd18d']]\n",
    "\n",
    "# Predictors\n",
    "kihd_predictors = data_kihd_preprocessed.drop(['tutknro', 'chdb16', 'chdb16d', 'amif16', 'amif16d', 'amig16', 'amig16d',\n",
    "       'amim16', 'amim16d', 'all16', 'all16d', 'cvd16', 'cvd16d', 'syd14', 'alzm14', 'vp14', 'kol14', 'diab14',\n",
    "       'ncvd16', 'ncvd16d', 'cv15', 'cv15d', 'all15', 'stro15', 'cvd15', 'chd15', 'amib15', 'isth15', 'hsth15', \n",
    "                                'db15', 'can15', 'canc15', 'ast15', 'copd15', 'dema15', 'finchd18', 'finchd18d'], axis = 1)\n",
    "\n",
    "# Separate genes and phenotypes\n",
    "genes_start = list(kihd_predictors.columns.values).index('FEDER2HH'.lower())\n",
    "genes_end = list(kihd_predictors.columns.values).index('CATAETT'.lower())\n",
    "\n",
    "# Genes only\n",
    "kihd_genes = kihd_predictors.iloc[:, genes_start:(genes_end+1)].copy()\n",
    "print(\"KIHD with genes only: {}\".format(kihd_genes.shape))\n",
    "\n",
    "# Phenotypes\n",
    "kihd_phenotypes = kihd_predictors.drop(kihd_predictors.columns.values[genes_start:genes_end+1], axis = 1)\n",
    "print(\"KIHD with phenotypes only: {}\".format(kihd_phenotypes.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Remove predictors and subjects based on the number of missing values in 'kihd_phenotypes'\n",
    "# -----------------------------------------\n",
    "# Remove variables (columns) containing more than 5% of missing values\n",
    "threshold_columns = kihd_phenotypes.shape[0]-round(0.05*kihd_phenotypes.shape[0])\n",
    "kihd_phenotypes = kihd_phenotypes.dropna(axis=1, thresh=threshold_columns) \n",
    "\n",
    "print(\"Filter out predictors with more than 5% of missing values ...\")\n",
    "print(\"Dataset size: {rows}x{cols}\".format(rows = kihd_phenotypes.shape[0], cols = kihd_phenotypes.shape[1]))\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "# Remove subjects (rows) with more than 5% of missing values\n",
    "threshold_rows = kihd_phenotypes.shape[1]-round(0.05*kihd_phenotypes.shape[1])\n",
    "kihd_phenotypes = kihd_phenotypes.dropna(axis=0, thresh=threshold_rows)\n",
    "\n",
    "print(\"Filter out rows with more than 5% of missing values ...\")\n",
    "print(\"Dataset size: {rows}x{cols}\".format(rows = kihd_phenotypes.shape[0], cols = kihd_phenotypes.shape[1]))\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "# Remove subjects in genes and outcomes correspondingly\n",
    "kihd_genes = kihd_genes.loc[kihd_phenotypes.index, :]\n",
    "kihd_outcomes = kihd_outcomes.loc[kihd_phenotypes.index, :]\n",
    "\n",
    "# Reset indices in all data frames after removing subjects \n",
    "kihd_genes = kihd_genes.reset_index(drop=True)\n",
    "kihd_outcomes = kihd_outcomes.reset_index(drop=True)\n",
    "kihd_phenotypes = kihd_phenotypes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Fill gaps in 'kihd_phenotypes' with kNN\n",
    "# -----------------------------------------\n",
    "\n",
    "# Scale predictors before applying the NN-based method\n",
    "scaler=MinMaxScaler().fit(kihd_phenotypes)\n",
    "kihd_phenotypes_scaled=scaler.transform(kihd_phenotypes)\n",
    "kihd_phenotypes_scaled_filled_knn = KNN(k=1).fit_transform(kihd_phenotypes_scaled)\n",
    "\n",
    "# Inverse scaling to original ranges\n",
    "kihd_phenotypes=pd.DataFrame(scaler.inverse_transform(kihd_phenotypes_scaled_filled_knn), columns=kihd_phenotypes.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Handle competing risks\n",
    "# -----------------------------------------\n",
    "# Remove subjects died because of any non-cardiovascular reason within the prediction horizon\n",
    "\n",
    "prediction_horizon = 35 * 365\n",
    "kihd_outcomes = kihd_outcomes.drop(kihd_outcomes[ (kihd_outcomes.loc[:, 'ncvd16'] == 1) & \n",
    "                                                 (kihd_outcomes.loc[:, 'ncvd16d'] <= prediction_horizon)].index, axis=0)\n",
    "# Remove subjects in kihd_genes and kihd_phenotypes correspondingly\n",
    "kihd_genes = kihd_genes.loc[kihd_outcomes.index, :]\n",
    "kihd_phenotypes = kihd_phenotypes.loc[kihd_outcomes.index, :]\n",
    "\n",
    "# Reset indices in all data frames after removing subjects \n",
    "kihd_genes = kihd_genes.reset_index(drop=True)\n",
    "kihd_outcomes = kihd_outcomes.reset_index(drop=True)\n",
    "kihd_phenotypes = kihd_phenotypes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Collect statistics for subjects in multiple runs of k-fold cross-validation\n",
    "# -----------------------------------------\n",
    "\n",
    "statistics_cv = pd.DataFrame({'TP': [0]*kihd_phenotypes.shape[0], 'TN': [0]*kihd_phenotypes.shape[0], \n",
    "                                    'FP': [0]*kihd_phenotypes.shape[0], 'FN': [0]*kihd_phenotypes.shape[0]})\n",
    "\n",
    "outcome = 'cvd16'\n",
    "\n",
    "data_x = kihd_phenotypes.copy()\n",
    "data_y = kihd_outcomes[[outcome]].values.ravel()\n",
    "\n",
    "true_y_test = np.array([])\n",
    "predicted_y_test = np.array([])\n",
    "\n",
    "true_y_train = np.array([])\n",
    "predicted_y_train = np.array([])\n",
    "\n",
    "selected_columns = []\n",
    "\n",
    "runs = 50\n",
    "selected_features = 0\n",
    "\n",
    "for r in range(runs):\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
    "    for train_index, test_index in skf.split(data_x, data_y):\n",
    "        data_x_train, data_x_test = data_x.iloc[train_index].copy(), data_x.iloc[test_index].copy()\n",
    "        y_train, y_test = data_y[train_index], data_y[test_index]\n",
    "        \n",
    "        #normalize\n",
    "        scaler = MinMaxScaler().fit(data_x_train)\n",
    "        data_x_train = scaler.transform(data_x_train)\n",
    "        data_x_test = scaler.transform(data_x_test)\n",
    "        \n",
    "        #train the model\n",
    "        model = LogisticRegression(penalty=\"l1\", max_iter=500, solver=\"liblinear\", C=0.15)\n",
    "        model.fit(data_x_train, y_train)\n",
    "        \n",
    "        coef = np.abs(np.array(model.coef_))\n",
    "        selected_columns.extend(list(kihd_phenotypes.columns[np.where(coef[0] > 0.0)]))\n",
    "        \n",
    "        selected_features += len(list(kihd_phenotypes.columns[np.where(coef[0] > 0.0)]))/(runs*5)\n",
    "        \n",
    "        class_counts = np.unique(y_train, return_counts = True)[1]\n",
    "        threshold_logit = np.min(class_counts)/np.sum(class_counts)\n",
    "        \n",
    "        #apply to test\n",
    "        y_prob_test = model.predict_proba(data_x_test)\n",
    "        predictions_test = [0 if risk[1] < threshold_logit else 1 for risk in y_prob_test]\n",
    "        predicted_y_test = np.append(predicted_y_test, predictions_test)\n",
    "        true_y_test = np.append(true_y_test, y_test)\n",
    "        \n",
    "        #apply to train\n",
    "        y_prob_train = model.predict_proba(data_x_train)\n",
    "        predictions_train = [0 if risk[1] < threshold_logit else 1 for risk in y_prob_train]\n",
    "        predicted_y_train = np.append(predicted_y_train, predictions_train)\n",
    "        true_y_train = np.append(true_y_train, y_train)\n",
    "        \n",
    "        for i, ind in enumerate(test_index):\n",
    "            if predictions_test[i] == y_test[i] == 1:\n",
    "                statistics_cv.loc[ind, 'TP'] += 1\n",
    "            elif predictions_test[i] == y_test[i] == 0:\n",
    "                statistics_cv.loc[ind, 'TN'] += 1\n",
    "            elif predictions_test[i] != y_test[i] == 0:\n",
    "                statistics_cv.loc[ind, 'FP'] += 1\n",
    "            elif predictions_test[i] != y_test[i] == 1:\n",
    "                statistics_cv.loc[ind, 'FN'] += 1 \n",
    "\n",
    "selected_columns = list(set(selected_columns))\n",
    "print(len(selected_columns))\n",
    "print(\"Test data\")\n",
    "print(\"Accuracy: {}\".format(accuracy_score(true_y_test, predicted_y_test)))\n",
    "M = confusion_matrix(true_y_test, predicted_y_test)\n",
    "print(M)\n",
    "print(\"TPR: {}\".format(M[1,1]/(M[1,0] + M[1,1])))\n",
    "print(\"TNR: {}\".format(M[0,0]/(M[0,0] + M[0,1])))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training data\")\n",
    "print(\"Accuracy: {}\".format(accuracy_score(true_y_train, predicted_y_train)))\n",
    "M = confusion_matrix(true_y_train, predicted_y_train)\n",
    "print(M)\n",
    "print(\"TPR: {}\".format(M[1,1]/(M[1,0] + M[1,1])))\n",
    "print(\"TNR: {}\".format(M[0,0]/(M[0,0] + M[0,1])))\n",
    "\n",
    "statistics_cv.loc[:, 'accuracy'] = (statistics_cv.loc[:, 'TP'] + statistics_cv.loc[:, 'TN']) / statistics_cv.sum(axis = 1)\n",
    "\n",
    "# Keep variables selected by the model at least once\n",
    "kihd_phenotypes = kihd_phenotypes.loc[:, selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Create intervals or get levels of variables\n",
    "# -----------------------------------------\n",
    "\n",
    "def discretize_kmeans(data):\n",
    "    \n",
    "    data_discretized = pd.DataFrame(columns = data.columns)\n",
    "    discrete_levels = {}\n",
    "    levels_number = []\n",
    "    \n",
    "    for i, col in enumerate(data.columns):\n",
    "        \n",
    "        unique_levels = np.unique(data.loc[:, col])\n",
    "        \n",
    "        if len(unique_levels) <= 2:\n",
    "            data_discretized.loc[:, col] = data.loc[:, col]\n",
    "            discrete_levels[col] = [(lvl, lvl) for lvl in unique_levels]\n",
    "            levels_number.append(2)\n",
    "            \n",
    "        elif 2 < len(unique_levels) <= 7:           \n",
    "            data_discretized.loc[:, col] = [np.where(unique_levels == item)[0] for item in data.loc[:, col]]\n",
    "            discrete_levels[col] = [(i, lvl) for i, lvl in enumerate(unique_levels)]\n",
    "            levels_number.append(len(unique_levels))\n",
    "          \n",
    "        else:\n",
    "            n_bins=4\n",
    "            discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='kmeans')\n",
    "            discretizer.fit(data.loc[:, col].values.reshape(-1, 1))\n",
    "            levels_kmeans = discretizer.transform(data.loc[:, col].values.reshape(-1, 1)).ravel()\n",
    "            data_discretized.loc[:, col] = levels_kmeans\n",
    "            \n",
    "            levels_number.append(len(np.unique(levels_kmeans)))\n",
    "            discrete_levels[col] = []\n",
    "            for j in range(len(np.unique(levels_kmeans))):\n",
    "                interval = np.unique(data.loc[np.where(levels_kmeans == j)[0], col])\n",
    "                discrete_levels[col].append((j, '[{}, {}]'.format(min(interval), max(interval))))\n",
    "            \n",
    "    return data_discretized, discrete_levels, levels_number\n",
    "\n",
    "# -----------------------------------------\n",
    "data_discretized, discrete_levels, levels_number = discretize_kmeans(kihd_phenotypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Define an objective function\n",
    "# -----------------------------------------\n",
    "def fintess(x):\n",
    "    return kihd_objective(x, levels_number.copy(), data_discretized.copy())\n",
    "\n",
    "\n",
    "def kihd_objective(x, levels_number, data_discretized):\n",
    "\n",
    "    bool_check_colomns = [True if x[i] < lvl else False for i, lvl in enumerate(levels_number)]\n",
    "\n",
    "    data_discretized_selected_cols = data_discretized.loc[:, bool_check_colomns]\n",
    "    levels_number_selected_cols = np.array(levels_number)[bool_check_colomns]\n",
    "    x_selected_cols = list(np.array(x)[bool_check_colomns])\n",
    "\n",
    "    if len(x_selected_cols) == 0:\n",
    "        return [100.0, 100.0, len(bool_check_colomns)] # if tpr and tnr are minimized \n",
    "        #return [0.0, 0.0, len(bool_check_colomns)] # if tpr and tnr are maximized \n",
    "    \n",
    "    selected = [True if x_selected_cols == list(row) else False for row in data_discretized_selected_cols.values]\n",
    "    selected = pd.Series(selected)\n",
    "\n",
    "    number_selected = statistics_cv.loc[selected, :].shape[0]\n",
    "\n",
    "    if number_selected == 0:\n",
    "        return [100.0, 100.0, len(selected.values)] # if tpr and tnr are minimized \n",
    "        #return [0.0, 0.0, len(selected.values)] # if tpr and tnr are maximized \n",
    "    \n",
    "    sick = (statistics_cv.loc[selected].sum()['TP'] + statistics_cv.loc[selected].sum()['FN'])\n",
    "    healthy = (statistics_cv.loc[selected].sum()['TN'] + statistics_cv.loc[selected].sum()['FP'])\n",
    "    \n",
    "    tpr = 100 * statistics_cv.loc[selected].sum()['TP']/sick if sick!=0 else 0\n",
    "    tnr = 100 * statistics_cv.loc[selected].sum()['TN']/healthy if healthy!=0 else 0\n",
    "\n",
    "    return [tpr, tnr, len(selected.values)-number_selected] # if tpr and tnr are minimized\n",
    "    #return [100-tpr, 100-tnr, len(selected.values)-number_selected] # if tpr and tnr are maximized  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Evaluate final solutions\n",
    "# -----------------------------------------\n",
    "\n",
    "def solution_evaluation(x, levels_number, data_discretized):\n",
    "\n",
    "    bool_check_colomns = [True if x[i] < lvl else False for i, lvl in enumerate(levels_number)]\n",
    "        \n",
    "    data_discretized_selected_cols = data_discretized.loc[:, bool_check_colomns]\n",
    "    levels_number_selected_cols = np.array(levels_number)[bool_check_colomns]\n",
    "    x_selected_cols = list(np.array(x)[bool_check_colomns])\n",
    "\n",
    "    if len(x_selected_cols) == 0:\n",
    "        return {'variables': '',\n",
    "            'tpr': 0.0, \n",
    "            'tnr': 0.0, \n",
    "            'number': 0,\n",
    "            'indices': np.nan}\n",
    "    \n",
    "    selected = [True if x_selected_cols == list(row) else False for row in data_discretized_selected_cols.values]\n",
    "    selected = pd.Series(selected)\n",
    "    \n",
    "    selected_indices = list(statistics_cv.loc[selected].index)\n",
    "\n",
    "    number_selected = statistics_cv.loc[selected, :].shape[0]\n",
    "\n",
    "    if number_selected == 0:\n",
    "        return {'variables': '',\n",
    "            'tpr': 0.0, \n",
    "            'tnr': 0.0, \n",
    "            'number': 0,\n",
    "            'indices': np.nan}\n",
    "    \n",
    "    sick = (statistics_cv.loc[selected].sum()['TP'] + statistics_cv.loc[selected].sum()['FN'])\n",
    "    healthy = (statistics_cv.loc[selected].sum()['TN'] + statistics_cv.loc[selected].sum()['FP'])\n",
    "    \n",
    "    tpr = 100 * statistics_cv.loc[selected].sum()['TP']/sick if sick!=0 else 0\n",
    "    tnr = 100 * statistics_cv.loc[selected].sum()['TN']/healthy if healthy!=0 else 0\n",
    " \n",
    "    group_variables = ['{}: {}  '.format(col, discrete_levels[col][x_selected_cols[i]]) for i, col in enumerate(data_discretized_selected_cols.columns.values)]\n",
    "    return {'variables': group_variables,\n",
    "            'tpr': tpr, \n",
    "            'tnr': tnr, \n",
    "            'number': number_selected,\n",
    "            'indices': selected_indices}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Generate an initial population\n",
    "# -----------------------------------------\n",
    "\n",
    "max_border_of_zero_elements = 2**np.ceil(np.log2(np.array(levels_number) + 1)) - 1\n",
    "min_border_of_zero_elements = np.array(levels_number)\n",
    "\n",
    "\n",
    "def generate_int_solution():\n",
    "    indv_int = []\n",
    "    for i in range(ndim):\n",
    "\n",
    "        gene_int_zero = np.random.randint(min_border_of_zero_elements[i], max_border_of_zero_elements[i] + 1, size=1)[0]\n",
    "        gene_int_not_zero = np.random.randint(min_border_of_zero_elements[i], size=1)[0]\n",
    "\n",
    "        indv_int.append(choice([gene_int_not_zero, gene_int_zero], 1, p = [0.05, 0.95])[0])\n",
    "        \n",
    "    return indv_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Run NSGAIII\n",
    "# -----------------------------------------\n",
    "\n",
    "from platypus.problems import Problem\n",
    "from platypus.algorithms import NSGAII, NSGAIII\n",
    "from platypus.types import Binary, Integer\n",
    "from platypus.core import Solution\n",
    "from numpy.random import choice\n",
    "from platypus.operators import InjectedPopulation\n",
    "from platypus.core import nondominated, Archive\n",
    "from platypus.tools import int2bin, bin2gray, bin2int, gray2bin\n",
    "\n",
    "nondominated_solutions = Archive()\n",
    "solutions_list = []\n",
    "\n",
    "ndim = len(levels_number)\n",
    "\n",
    "for r in range(1):\n",
    "    problem = Problem(len(levels_number), 3)\n",
    "    problem.types = [Integer(0, 2**np.ceil(np.log2(max_value + 1)) - 1) for max_value in levels_number]\n",
    "           \n",
    "    problem.function = fintess\n",
    "\n",
    "    problem.directions[0] = Problem.MINIMIZE\n",
    "    problem.directions[1] = Problem.MINIMIZE\n",
    "    problem.directions[2] = Problem.MINIMIZE\n",
    "\n",
    "    divisions_outer=20\n",
    "    algorithm = NSGAIII(problem, divisions_outer=divisions_outer)\n",
    "    \n",
    "    population_size = algorithm.population_size\n",
    "      \n",
    "    init_pop = [Solution(problem) for i in range(population_size)]\n",
    "    pop_indiv = []\n",
    "    for i in range(population_size):\n",
    "        indv_int = generate_int_solution()\n",
    "        indv = []\n",
    "        for j in range(ndim):\n",
    "            indv.append(bin2gray(int2bin(indv_int[j], np.ceil(np.log2(levels_number[j] + 1)))))\n",
    "        pop_indiv.append(indv)\n",
    "    \n",
    "    for i in range(population_size):\n",
    "        init_pop[i].variables = pop_indiv[i]\n",
    "\n",
    "\n",
    "    algorithm = NSGAIII(problem, divisions_outer=divisions_outer, generator=InjectedPopulation(init_pop))\n",
    "    generations = 5\n",
    "    algorithm.run(population_size * generations)\n",
    "\n",
    "    solutions_list.extend(algorithm.result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Save all the rules generated\n",
    "# -----------------------------------------\n",
    "\n",
    "rules = pd.DataFrame(columns = ['variables', 'tpr', 'tnr', 'number', 'indices'])\n",
    "for s in solutions_list:\n",
    "    int_sol = [bin2int(gray2bin(s.variables[i])) for i in range(ndim)]\n",
    "    rules = rules.append(solution_evaluation(int_sol, levels_number, data_discretized), ignore_index=True) \n",
    "    \n",
    "rules.loc[:, 'variables_str'] = [''.join(rules.loc[i, 'variables']) for i in range(rules.shape[0])]\n",
    "\n",
    "rules_unique = rules.iloc[np.where(~pd.DataFrame(rules[['variables_str']]).duplicated().values == True)[0]].reset_index(drop=True)\n",
    "\n",
    "rules_unique.to_csv('lasso_c0.15_runs50_min.csv', index = False) # if tpr and tnr are minimized\n",
    "#rules_unique.to_csv('lasso_c0.15_runs50_min.csv', index = False) # if tpr and tnr are maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULE PROCESSING\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# If there are equal sets, remove\n",
    "# -----------------------------------------\n",
    "def remove_equal_sets(nondominated_objectives_reduced):\n",
    "\n",
    "    equal_sets = dict((ind,[]) for ind in nondominated_objectives_reduced.index)\n",
    "\n",
    "    for ind1 in nondominated_objectives_reduced.index:\n",
    "\n",
    "        for ind2 in nondominated_objectives_reduced.index[(ind1+1):]:\n",
    "            set1 = set(nondominated_objectives_reduced.loc[ind1, 'indices'])\n",
    "            set2 = set(nondominated_objectives_reduced.loc[ind2, 'indices'])\n",
    "\n",
    "            if set1.issubset(set2) and set2.issubset(set1):\n",
    "                equal_sets[ind1].append(ind2)\n",
    "                equal_sets[ind2].append(ind1)\n",
    "\n",
    "    # remove duplicates\n",
    "    for ind1 in nondominated_objectives_reduced.index:\n",
    "        if len(equal_sets[ind1]) != 0:\n",
    "            #equal_sets[ind1].append(ind1)\n",
    "            groups = equal_sets[ind1]\n",
    "\n",
    "            for gr in groups:\n",
    "                set1 = set(nondominated_objectives_reduced.loc[ind1, 'variables'])\n",
    "                set2 = set(nondominated_objectives_reduced.loc[gr, 'variables'])\n",
    "\n",
    "                nondominated_objectives_reduced.loc[ind1, 'variables'].extend(list(set2 - set1))\n",
    "\n",
    "            nondominated_objectives_reduced = nondominated_objectives_reduced.drop(groups, axis = 0)\n",
    "\n",
    "            for ind2 in equal_sets[ind1]:\n",
    "                equal_sets[ind2] = []\n",
    "\n",
    "            equal_sets[ind1] = []\n",
    "\n",
    "    nondominated_objectives_reduced = nondominated_objectives_reduced.reset_index(drop=True)\n",
    "    \n",
    "    return nondominated_objectives_reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Read rules from files and keep only unique ones\n",
    "# -----------------------------------------\n",
    "\n",
    "from ast import literal_eval\n",
    "objectives_reduced_max = pd.read_csv('lasso_c0.15_runs50_max.csv', sep = ',', converters={'variables': literal_eval, 'indices': literal_eval})\n",
    "objectives_reduced_min = pd.read_csv('lasso_c0.15_runs50_min.csv', sep = ',', converters={'variables': literal_eval, 'indices': literal_eval})\n",
    "\n",
    "objectives_reduced_max = objectives_reduced_max.iloc[np.where(~pd.DataFrame(objectives_reduced_max[['variables_str']]).duplicated().values == True)[0]].reset_index(drop=True)\n",
    "objectives_reduced_min = objectives_reduced_min.iloc[np.where(~pd.DataFrame(objectives_reduced_min[['variables_str']]).duplicated().values == True)[0]].reset_index(drop=True)\n",
    "\n",
    "objectives_reduced_max = objectives_reduced_max.drop('variables_str', axis = 1)\n",
    "objectives_reduced_min = objectives_reduced_min.drop('variables_str', axis = 1)\n",
    "\n",
    "objectives_reduced_max = remove_equal_sets(objectives_reduced_max)\n",
    "objectives_reduced_min = remove_equal_sets(objectives_reduced_min)\n",
    "\n",
    "\n",
    "all_groups = pd.DataFrame()\n",
    "all_groups = pd.concat([objectives_reduced_max.copy(), objectives_reduced_min.copy()]).reset_index(drop=True)\n",
    "\n",
    "all_groups = remove_equal_sets(all_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Estimate accuracy for each rule\n",
    "# -----------------------------------------\n",
    "average_accuracy = statistics_cv.loc[:, 'accuracy'].mean()\n",
    "all_groups.loc[:, 'accuracy'] = [ statistics_cv.loc[all_groups.loc[i, 'indices'], 'accuracy'].mean() for i in range(all_groups.shape[0])]\n",
    "all_groups.loc[:, 'level'] = [ 'lower' if all_groups.loc[i, 'accuracy'] < average_accuracy else 'higher' for i in range(all_groups.shape[0])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Select rules using thresholds\n",
    "# -----------------------------------------\n",
    "tpr_tnr_difference = (all_groups.loc[:, ['tpr', 'tnr']].max(axis=1) - all_groups.loc[:, ['tpr', 'tnr']].min(axis=1))/all_groups.loc[:, ['tpr', 'tnr']].max(axis=1) < 0.1\n",
    "min_group_size = all_groups.loc[:, 'number'] >= 30\n",
    "\n",
    "max_acc_threshold = 0.95\n",
    "min_acc_threshold = 0.50\n",
    "all_groups_reduced = all_groups.loc[ min_group_size & tpr_tnr_difference & ( (all_groups.loc[:, 'accuracy'] >= max_acc_threshold) | (all_groups.loc[:, 'accuracy'] <= min_acc_threshold) ) ].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Remove subsets\n",
    "# -----------------------------------------\n",
    "\n",
    "def remove_subsets(nondominated_objectives_reduced):\n",
    "    # if there are subsets\n",
    "    subsets = dict((ind,[]) for ind in nondominated_objectives_reduced.index)\n",
    "\n",
    "    for ind1 in nondominated_objectives_reduced.index:\n",
    "\n",
    "        for ind2 in nondominated_objectives_reduced.index:\n",
    "            set1 = set(nondominated_objectives_reduced.loc[ind1, 'indices'])\n",
    "            set2 = set(nondominated_objectives_reduced.loc[ind2, 'indices'])\n",
    "\n",
    "            if set1.issubset(set2) and ind1!=ind2:\n",
    "                subsets[ind1].append(ind2)\n",
    "\n",
    "    # remove subsets\n",
    "    for ind1 in nondominated_objectives_reduced.index:\n",
    "        if len(subsets[ind1]) != 0:\n",
    "            nondominated_objectives_reduced = nondominated_objectives_reduced.drop(ind1, axis = 0)\n",
    "\n",
    "        for ind2 in nondominated_objectives_reduced.index:\n",
    "            if ind1 in subsets[ind2]:\n",
    "                subsets[ind2].remove(ind1) \n",
    "\n",
    "            subsets[ind1] = []\n",
    "            \n",
    "    return nondominated_objectives_reduced.reset_index(drop=True)\n",
    "\n",
    "all_groups_reduced = remove_subsets(all_groups_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Check which rules (for easy or diffucult cases) cover each subject\n",
    "# -----------------------------------------\n",
    "def get_subjects_in_groups(all_groups_reduced):\n",
    "    \n",
    "    subjects_in_groups = pd.DataFrame({'lower': [0]*len(kihd_phenotypes), 'higher': [0]*len(kihd_phenotypes)})\n",
    "\n",
    "    for i in range(all_groups_reduced.shape[0]):\n",
    "        subjects_in_groups.loc[all_groups_reduced.loc[i, 'indices'], all_groups_reduced.loc[i, 'level']] += 1\n",
    "\n",
    "    subjects_in_groups.loc[:, 'accuracy'] = statistics_cv.loc[:, 'accuracy']\n",
    "    \n",
    "    return subjects_in_groups\n",
    "\n",
    "subjects_in_groups = get_subjects_in_groups(all_groups_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(751, 3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects_in_groups.loc[(subjects_in_groups.loc[:, 'lower'] == 0) & (subjects_in_groups.loc[:, 'higher'] == 0)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Get statistics for the chosen thresholds\n",
    "# -----------------------------------------\n",
    "def get_cases_number(subjects_in_groups):\n",
    "        \n",
    "    easy_cases = 0\n",
    "    difficult_cases = 0\n",
    "    mixed_cases = 0\n",
    "    not_covered_cases = 0\n",
    "\n",
    "    easiness = []\n",
    "    \n",
    "    accuracy_not_covered = 0.\n",
    "    accuracy_mixed = 0.\n",
    "    accuracy_easy = 0.\n",
    "    accuracy_difficult = 0.\n",
    "\n",
    "    for i in range(subjects_in_groups.shape[0]):\n",
    "        if subjects_in_groups.loc[i, 'lower'] == 0 and subjects_in_groups.loc[i, 'higher'] == 0:\n",
    "            not_covered_cases += 1\n",
    "            easiness.append('not covered')\n",
    "            accuracy_not_covered += subjects_in_groups.loc[i, 'accuracy']\n",
    "        else:\n",
    "            if subjects_in_groups.loc[i, 'lower'] > 0 and subjects_in_groups.loc[i, 'higher'] > 0:\n",
    "                mixed_cases += 1\n",
    "                easiness.append('ambiguous case')\n",
    "                accuracy_mixed += subjects_in_groups.loc[i, 'accuracy']\n",
    "            elif subjects_in_groups.loc[i, 'lower'] > 0 and subjects_in_groups.loc[i, 'higher'] == 0:\n",
    "                difficult_cases += 1\n",
    "                easiness.append('difficult case')\n",
    "                accuracy_difficult += subjects_in_groups.loc[i, 'accuracy']\n",
    "            elif subjects_in_groups.loc[i, 'lower'] == 0 and subjects_in_groups.loc[i, 'higher'] > 0:\n",
    "                easy_cases += 1\n",
    "                easiness.append('easy case')\n",
    "                accuracy_easy += subjects_in_groups.loc[i, 'accuracy']\n",
    "\n",
    "    if easy_cases>0:\n",
    "        accuracy_easy = accuracy_easy/easy_cases\n",
    "        \n",
    "    if difficult_cases>0:\n",
    "        accuracy_difficult = accuracy_difficult/difficult_cases\n",
    "        \n",
    "    if not_covered_cases>0:\n",
    "        accuracy_not_covered = accuracy_not_covered/not_covered_cases\n",
    "        \n",
    "    if mixed_cases>0:\n",
    "        accuracy_mixed = accuracy_mixed/mixed_cases\n",
    "        \n",
    "        \n",
    "    \n",
    "    return not_covered_cases, mixed_cases, difficult_cases, easy_cases, easiness, accuracy_not_covered, accuracy_mixed, accuracy_easy, accuracy_difficult  \n",
    "\n",
    "\n",
    "easiness = get_cases_number(subjects_in_groups)[4]\n",
    "easiness_selected = [item for item in easiness if item!='not covered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Subjects in the rule space\n",
    "# -----------------------------------------\n",
    "clustering_data = pd.DataFrame(0, columns = all_groups_reduced.index, index = kihd_phenotypes.index)\n",
    "\n",
    "for i in range(all_groups_reduced.shape[0]):\n",
    "    for ind in all_groups_reduced.loc[i, 'indices']:\n",
    "        clustering_data.loc[ind, i] = 1\n",
    "        \n",
    "clustering_data = clustering_data.loc[~(clustering_data==0).all(axis=1)]\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "gene_groups_tsne = TSNE(n_components=2, metric='euclidean', perplexity=30.0).fit_transform(clustering_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Use SVM to draw a border between easy and difficult\n",
    "# Use kmeans to find subclucters within easy and difficult groups\n",
    "# -----------------------------------------\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X = gene_groups_tsne[np.array(easiness_selected) != 'ambiguous case']\n",
    "y = np.array(easiness_selected)[np.array(easiness_selected) != 'ambiguous case']\n",
    "\n",
    "model = SVC(C=1.0, kernel='rbf', degree=3, gamma=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "easy_difficult_predictions = model.predict(X)\n",
    "\n",
    "n_clusters_easy = 10\n",
    "kmeans_easy = KMeans(n_clusters=n_clusters_easy, random_state=None).fit(gene_groups_tsne[np.array(easiness_selected) == 'easy case'])\n",
    "\n",
    "n_clusters_difficult = 6\n",
    "kmeans_difficult = KMeans(n_clusters=n_clusters_difficult, random_state=None).fit(gene_groups_tsne[np.array(easiness_selected) == 'difficult case'])\n",
    "\n",
    "kmeans_labels = np.repeat(-1, gene_groups_tsne.shape[0])\n",
    "kmeans_labels[np.where(np.array(easiness_selected) == 'easy case')] = kmeans_easy.predict(gene_groups_tsne[np.array(easiness_selected) == 'easy case'])\n",
    "kmeans_labels[np.where(np.array(easiness_selected) == 'difficult case')] = kmeans_difficult.predict(gene_groups_tsne[np.array(easiness_selected) == 'difficult case']) + n_clusters_easy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Put the result into a color plot\n",
    "# -----------------------------------------\n",
    "\n",
    "h = .1    \n",
    "\n",
    "x_min, x_max = gene_groups_tsne[:, 0].min() - 5, gene_groups_tsne[:, 0].max() + 5\n",
    "y_min, y_max = gene_groups_tsne[:, 1].min() - 5, gene_groups_tsne[:, 1].max() + 5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z[np.where(Z == 'easy case')] = kmeans_easy.predict(np.c_[xx.ravel(), yy.ravel()][np.where(Z == 'easy case')])\n",
    "Z[np.where(Z == 'difficult case')] = kmeans_difficult.predict(np.c_[xx.ravel(), yy.ravel()][np.where(Z == 'difficult case')]) + n_clusters_easy\n",
    "\n",
    "Z = Z.astype('int')\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.subplots(figsize=(20,10))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='none',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.PiYG, alpha = 0.25, \n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "colors = []\n",
    "for item in easiness_selected:\n",
    "    if item == 'ambiguous case':\n",
    "        colors.append('black')\n",
    "    elif item == 'easy case':\n",
    "        colors.append('green')\n",
    "    else:\n",
    "        colors.append('red')\n",
    "        \n",
    "values = []\n",
    "for item in easiness_selected:\n",
    "    if item == 'ambiguous case':\n",
    "        values.append(0)\n",
    "    elif item == 'easy case':\n",
    "        values.append(1)\n",
    "    else:\n",
    "        values.append(2)\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "colormap = np.array(['black', 'green', 'red', 'blue'])\n",
    "\n",
    "plt.scatter(gene_groups_tsne[:, 0], gene_groups_tsne[:, 1], c=colormap[values], s = 10, alpha = 0.5)\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "\n",
    "centroids_kmeans_easy = kmeans_easy.cluster_centers_\n",
    "centroids_kmeans_difficult = kmeans_difficult.cluster_centers_\n",
    "centroids = np.concatenate([centroids_kmeans_easy, centroids_kmeans_difficult])\n",
    "\n",
    "plt.scatter(centroids_kmeans_easy[:, 0], centroids_kmeans_easy[:, 1],\n",
    "            marker='*', s=30, linewidths=3.,\n",
    "            c=colormap[3], zorder=5)\n",
    "\n",
    "plt.scatter(centroids_kmeans_difficult[:, 0], centroids_kmeans_difficult[:, 1],\n",
    "            marker='*', s=30, linewidths=3.,\n",
    "            c=colormap[3], zorder=5)\n",
    "\n",
    "pop_a = mpatches.Patch(color='black', label='ambiguous case')\n",
    "pop_b = mpatches.Patch(color='green', label='easy case')\n",
    "pop_c = mpatches.Patch(color='red', label='difficult case')\n",
    "pop_d = mpatches.Patch(color='blue', label='centroid')\n",
    "\n",
    "plt.legend(handles = [pop_a, pop_b, pop_c, pop_d])\n",
    "\n",
    "not_ambiguous_case = np.array(easiness_selected) != 'ambiguous case'\n",
    "\n",
    "sick_tpr = np.array([])\n",
    "healthy_tnr = np.array([])\n",
    "for cl in range(n_clusters_easy + n_clusters_difficult):\n",
    "    sick = kihd_outcomes.loc[clustering_data.loc[(kmeans_labels == cl) & not_ambiguous_case].index, 'cvd16'].sum()\n",
    "    healthy = len(clustering_data.loc[(kmeans_labels == cl) & not_ambiguous_case].index) - sick\n",
    "    \n",
    "    items = kihd_outcomes.loc[clustering_data.loc[(kmeans_labels == cl) & not_ambiguous_case].index].index\n",
    "    \n",
    "    s_tpr = subjects_in_groups.loc[kihd_outcomes.loc[items][kihd_outcomes.loc[items, 'cvd16'] == 1].index, 'accuracy'].mean()*100\n",
    "    h_tnr = subjects_in_groups.loc[kihd_outcomes.loc[items][kihd_outcomes.loc[items, 'cvd16'] == 0].index, 'accuracy'].mean()*100\n",
    "\n",
    "    sick_tpr = np.append(sick_tpr, [sick, s_tpr])\n",
    "    healthy_tnr = np.append(healthy_tnr, [healthy, h_tnr])\n",
    "    \n",
    "sick_tpr = sick_tpr.reshape(-1, 2)\n",
    "healthy_tnr = healthy_tnr.reshape(-1, 2)\n",
    "\n",
    "accuracy_clusters = np.array([])\n",
    "for cl in range(n_clusters_easy + n_clusters_difficult):\n",
    "    accuracy_clusters = np.append(accuracy_clusters, subjects_in_groups.loc[clustering_data.loc[kmeans_labels == cl].index, 'accuracy'].mean())\n",
    "\n",
    "for cl in range(n_clusters_easy + n_clusters_difficult):\n",
    "    plt.text(centroids[cl, 0], centroids[cl, 1] + 1, \"{}%\".format(round(accuracy_clusters[cl]*100, 2)),\n",
    "             fontsize = 15, color='blue', \n",
    "             horizontalalignment='center', verticalalignment='bottom', style = 'normal')\n",
    "\n",
    "for cl in range(n_clusters_easy + n_clusters_difficult):\n",
    "    plt.text(centroids[cl, 0], centroids[cl, 1] - 2, \"no cvd: {:.0f} -> {:.2f}%\\ncvd: {:.0f} -> {:.2f}%\".format(healthy_tnr[cl][0], healthy_tnr[cl][1],\n",
    "             sick_tpr[cl][0], sick_tpr[cl][1]),\n",
    "             fontsize = 10, color='blue', \n",
    "             horizontalalignment='center', verticalalignment='top', style = 'normal')\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
